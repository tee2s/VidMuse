{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57efc0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 2\n",
      "Num Worker: 0\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "cfg = OmegaConf.load(\"/work/users/t/i/tis/VidMuse/output/VidMuse/xps/31a205cf/.hydra/config.yaml\")\n",
    "cfg.dataset.batch_size = 2\n",
    "cfg.dataset.num_workers = 0\n",
    "print(\"Batch Size:\", cfg.dataset.batch_size)\n",
    "print(\"Num Worker:\", cfg.dataset.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02ebf027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "dtype: float32\n",
      "autocast: true\n",
      "autocast_dtype: float16\n",
      "seed: 2036\n",
      "show: false\n",
      "continue_from: //pretrained/facebook/musicgen-small\n",
      "execute_only: null\n",
      "execute_inplace: false\n",
      "benchmark_no_load: false\n",
      "efficient_attention_backend: torch\n",
      "num_threads: 1\n",
      "mp_start_method: forkserver\n",
      "label: null\n",
      "logging:\n",
      "  level: INFO\n",
      "  log_updates: 1\n",
      "  log_tensorboard: true\n",
      "  log_wandb: true\n",
      "tensorboard:\n",
      "  with_media_logging: false\n",
      "  name: null\n",
      "  sub_dir: null\n",
      "wandb:\n",
      "  with_media_logging: true\n",
      "  project: VideoMusicGeneration\n",
      "  name: default\n",
      "  group: default\n",
      "slurm:\n",
      "  gpus: 2\n",
      "  mem_per_gpu: 32\n",
      "  time: 600\n",
      "  constraint: null\n",
      "  partition: l40-gpu\n",
      "  comment: null\n",
      "  setup:\n",
      "  - source ~/.bashrc\n",
      "  - echo \"FFMPEG located at $(which ffmpeg)\"\n",
      "  - echo \"$(ffmpeg -version)\"\n",
      "  - cd /work/users/t/i/tis/VidMuse\n",
      "  - source .venv/bin/activate\n",
      "  exclude: null\n",
      "  qos: gpu_access\n",
      "  cpus_per_gpu: 4\n",
      "  one_task_per_node: false\n",
      "dora:\n",
      "  dir: /checkpoint/tis/experiments/audiocraft/outputs\n",
      "  exclude:\n",
      "  - device\n",
      "  - wandb.*\n",
      "  - tensorboard.*\n",
      "  - logging.*\n",
      "  - dataset.num_workers\n",
      "  - eval.num_workers\n",
      "  - special.*\n",
      "  - metrics.visqol.bin\n",
      "  - metrics.fad.bin\n",
      "  - execute_only\n",
      "  - execute_best\n",
      "  - generate.every\n",
      "  - optim.eager_sync\n",
      "  - profiler.*\n",
      "  - deadlock.*\n",
      "  - efficient_attention_backend\n",
      "  - num_threads\n",
      "  - mp_start_method\n",
      "  use_rendezvous: false\n",
      "  git_save: true\n",
      "datasource:\n",
      "  max_sample_rate: 44100\n",
      "  max_channels: 2\n",
      "  train: egs/V2M20K/train\n",
      "  valid: egs/V2M20K/valid\n",
      "  evaluate: egs/V2M20K/eval\n",
      "  generate: egs/V2M20K/eval\n",
      "solver: musicgen\n",
      "fsdp:\n",
      "  use: false\n",
      "  param_dtype: float16\n",
      "  reduce_dtype: float32\n",
      "  buffer_dtype: float32\n",
      "  sharding_strategy: shard_grad_op\n",
      "  per_block: true\n",
      "profiler:\n",
      "  enabled: false\n",
      "deadlock:\n",
      "  use: true\n",
      "  timeout: 600\n",
      "dataset:\n",
      "  batch_size: 2\n",
      "  num_workers: 0\n",
      "  segment_duration: 29\n",
      "  num_samples: null\n",
      "  return_info: true\n",
      "  shuffle: false\n",
      "  sample_on_duration: false\n",
      "  sample_on_weight: false\n",
      "  min_segment_ratio: 0.8\n",
      "  train:\n",
      "    num_samples: 2\n",
      "    shuffle: true\n",
      "    shuffle_seed: 0\n",
      "    permutation_on_files: false\n",
      "    merge_text_p: 0.25\n",
      "    drop_desc_p: 0.5\n",
      "    drop_other_p: 0.5\n",
      "  valid:\n",
      "    num_samples: 2\n",
      "  evaluate:\n",
      "    num_samples: 2\n",
      "  generate:\n",
      "    num_samples: 2\n",
      "    return_info: true\n",
      "checkpoint:\n",
      "  save_last: true\n",
      "  save_every: 1\n",
      "  keep_last: 1\n",
      "  keep_every_states: null\n",
      "generate:\n",
      "  every: 100\n",
      "  path: samples\n",
      "  audio:\n",
      "    format: wav\n",
      "    strategy: loudness\n",
      "    sample_rate: 32000\n",
      "    loudness_headroom_db: 14\n",
      "  lm:\n",
      "    use_sampling: true\n",
      "    temp: 1.0\n",
      "    top_k: 250\n",
      "    top_p: 0.0\n",
      "    prompted_samples: true\n",
      "    unprompted_samples: true\n",
      "    gen_gt_samples: false\n",
      "    prompt_duration: null\n",
      "    gen_duration: null\n",
      "    remove_prompts: false\n",
      "  num_workers: 5\n",
      "evaluate:\n",
      "  every: 1\n",
      "  num_workers: 5\n",
      "  truncate_audio: null\n",
      "  fixed_generation_duration: null\n",
      "  metrics:\n",
      "    base: false\n",
      "    fad: true\n",
      "    kld: true\n",
      "    text_consistency: false\n",
      "    chroma_cosine: true\n",
      "optim:\n",
      "  epochs: 10\n",
      "  updates_per_epoch: 10\n",
      "  lr: 3.5e-05\n",
      "  optimizer: adamw\n",
      "  adam:\n",
      "    betas:\n",
      "    - 0.9\n",
      "    - 0.95\n",
      "    weight_decay: 0.1\n",
      "    eps: 1.0e-08\n",
      "  ema:\n",
      "    use: true\n",
      "    updates: 10\n",
      "    device: cuda\n",
      "    decay: 0.99\n",
      "  max_norm: 1.0\n",
      "  eager_sync: true\n",
      "schedule:\n",
      "  lr_scheduler: cosine\n",
      "  step:\n",
      "    step_size: null\n",
      "    gamma: null\n",
      "  exponential:\n",
      "    lr_decay: null\n",
      "  cosine:\n",
      "    warmup: 4000\n",
      "    lr_min_ratio: 0.0\n",
      "    cycle_length: 1.0\n",
      "  polynomial_decay:\n",
      "    warmup: null\n",
      "    zero_lr_warmup_steps: 0\n",
      "    end_lr: 0.0\n",
      "    power: 1\n",
      "  inverse_sqrt:\n",
      "    warmup: null\n",
      "    warmup_init_lr: 0.0\n",
      "  linear_warmup:\n",
      "    warmup: null\n",
      "    warmup_init_lr: 0.0\n",
      "classifier_free_guidance:\n",
      "  training_dropout: 0.3\n",
      "  inference_coef: 3.0\n",
      "attribute_dropout: {}\n",
      "fuser:\n",
      "  cross_attention_pos_emb: false\n",
      "  cross_attention_pos_emb_scale: 1\n",
      "  sum: []\n",
      "  prepend: []\n",
      "  cross:\n",
      "  - description\n",
      "  input_interpolate: []\n",
      "conditioners:\n",
      "  description:\n",
      "    model: t5\n",
      "    t5:\n",
      "      name: t5-base\n",
      "      finetune: false\n",
      "      word_dropout: 0.3\n",
      "      normalize_text: false\n",
      "sample_rate: 32000\n",
      "channels: 1\n",
      "compression_model_checkpoint: //pretrained/facebook/encodec_32khz\n",
      "compression_model_n_q: null\n",
      "tokens:\n",
      "  padding_with_special_token: false\n",
      "interleave_stereo_codebooks:\n",
      "  use: false\n",
      "  per_timestep: false\n",
      "cache:\n",
      "  path: null\n",
      "  write: false\n",
      "  write_shard: 0\n",
      "  write_num_shards: 1\n",
      "metrics:\n",
      "  fad:\n",
      "    use_gt: false\n",
      "    model: tf\n",
      "    tf:\n",
      "      bin: null\n",
      "      model_path: //reference/fad/vggish_model.ckpt\n",
      "  kld:\n",
      "    use_gt: false\n",
      "    model: passt\n",
      "    passt:\n",
      "      pretrained_length: 20\n",
      "  text_consistency:\n",
      "    use_gt: false\n",
      "    model: clap\n",
      "    clap:\n",
      "      model_path: //reference/clap/music_audioset_epoch_15_esc_90.14.pt\n",
      "      model_arch: HTSAT-base\n",
      "      enable_fusion: false\n",
      "  chroma_cosine:\n",
      "    use_gt: false\n",
      "    model: chroma_base\n",
      "    chroma_base:\n",
      "      sample_rate: 32000\n",
      "      n_chroma: 12\n",
      "      radix2_exp: 14\n",
      "      argmax: true\n",
      "lm_model: transformer_lm\n",
      "codebooks_pattern:\n",
      "  modeling: delay\n",
      "  delay:\n",
      "    delays:\n",
      "    - 0\n",
      "    - 1\n",
      "    - 2\n",
      "    - 3\n",
      "    flatten_first: 0\n",
      "    empty_initial: 0\n",
      "  unroll:\n",
      "    flattening:\n",
      "    - 0\n",
      "    - 1\n",
      "    - 2\n",
      "    - 3\n",
      "    delays:\n",
      "    - 0\n",
      "    - 0\n",
      "    - 0\n",
      "    - 0\n",
      "  music_lm:\n",
      "    group_by: 2\n",
      "  coarse_first:\n",
      "    delays:\n",
      "    - 0\n",
      "    - 0\n",
      "    - 0\n",
      "transformer_lm:\n",
      "  dim: 1024\n",
      "  num_heads: 16\n",
      "  num_layers: 24\n",
      "  hidden_scale: 4\n",
      "  n_q: 4\n",
      "  card: 2048\n",
      "  dropout: 0.0\n",
      "  emb_lr: null\n",
      "  activation: gelu\n",
      "  norm_first: true\n",
      "  bias_ff: false\n",
      "  bias_attn: false\n",
      "  bias_proj: false\n",
      "  past_context: null\n",
      "  causal: true\n",
      "  custom: false\n",
      "  memory_efficient: true\n",
      "  attention_as_float32: false\n",
      "  layer_scale: null\n",
      "  positional_embedding: sin\n",
      "  xpos: false\n",
      "  checkpointing: none\n",
      "  weight_init: gaussian\n",
      "  depthwise_init: current\n",
      "  zero_bias_init: true\n",
      "  norm: layer_norm\n",
      "  cross_attention: false\n",
      "  qk_layer_norm: false\n",
      "  qk_layer_norm_cross: false\n",
      "  attention_dropout: null\n",
      "  kv_repeat: 1\n",
      "  two_step_cfg: false\n",
      "video:\n",
      "  visual_encoder: clip\n",
      "  video_fps: 2\n",
      "  video_overlap: 0\n",
      "  add_global:\n",
      "    if_add_gobal: true\n",
      "    global_feature_path: ''\n",
      "    mode: average\n",
      "    num_frames: 64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(cfg, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd15d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_workers in builder: 0\n",
      "Using 0 and 2 for data loading\n",
      "num_workers in builder: 0\n",
      "Using 0 and 2 for data loading\n",
      "num_workers in builder: 0\n",
      "Using 0 and 2 for data loading\n",
      "num_workers in builder: 0\n",
      "Using 0 and 2 for data loading\n",
      "dict_keys(['train', 'valid', 'evaluate', 'generate'])\n"
     ]
    }
   ],
   "source": [
    "from audiocraft.solvers.builders import get_audio_datasets, DatasetType\n",
    "dataloaders = get_audio_datasets(cfg, dataset_type=DatasetType.MUSIC)\n",
    "print(dataloaders.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30f3a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/users/t/i/tis/V2Music/preprocessing/data/video/jx0u5fNZECQ.mp4\n",
      "fps=25.00, total_frames=751\n",
      "video frames: 58\n",
      "flow frames: 117\n",
      "video_tensor shape torch.Size([3, 58, 224, 224])\n",
      "flow_tensor shape torch.Size([1, 29, 224, 224])\n",
      "loading from here\n",
      "Called getitem of InfoAudioDataset\n",
      "Called getitem in MusicDataset\n",
      "/work/users/t/i/tis/V2Music/preprocessing/data/video/C0o1BhJJ1ZI.mp4\n",
      "fps=25.00, total_frames=751\n",
      "video frames: 58\n",
      "flow frames: 117\n",
      "video_tensor shape torch.Size([3, 58, 224, 224])\n",
      "flow_tensor shape torch.Size([1, 29, 224, 224])\n",
      "loading from here\n",
      "Called getitem of InfoAudioDataset\n",
      "Called getitem in MusicDataset\n"
     ]
    }
   ],
   "source": [
    "#ld = dataloaders['train']\n",
    "ld = dataloaders['valid']\n",
    "it = iter(ld)\n",
    "batch = next(it)\n",
    "audio, video, info = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f3e4d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "147197f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'list'> <class 'list'>\n",
      "MusicInfo(meta=AudioMeta(path='/work/users/t/i/tis/V2Music/preprocessing/bg_audio/mdx_extra/jx0u5fNZECQ_no_vocals.wav', video_path='/work/users/t/i/tis/V2Music/preprocessing/data/video/jx0u5fNZECQ.mp4', duration=29.993514739229024, sample_rate=44100, amplitude=None, weight=None, info_path=None), seek_time=0, n_frames=928000, total_frames=928000, sample_rate=32000, channels=1, audio_tokens=None, title=None, artist=None, key=None, bpm=None, genre=None, moods=None, keywords=None, description=None, name=None, instrument=None, self_wav=WavCondition(wav=tensor([[[-0.0004, -0.0394, -0.0493,  ...,  0.0098,  0.0088,  0.0106]]]), length=tensor([928000]), sample_rate=[32000], path=['/work/users/t/i/tis/V2Music/preprocessing/bg_audio/mdx_extra/jx0u5fNZECQ_no_vocals.wav'], seek_time=[0]), joint_embed={})\n",
      "MusicInfo(meta=AudioMeta(path='/work/users/t/i/tis/V2Music/preprocessing/bg_audio/mdx_extra/C0o1BhJJ1ZI_no_vocals.wav', video_path='/work/users/t/i/tis/V2Music/preprocessing/data/video/C0o1BhJJ1ZI.mp4', duration=29.993514739229024, sample_rate=44100, amplitude=None, weight=None, info_path=None), seek_time=0, n_frames=928000, total_frames=928000, sample_rate=32000, channels=1, audio_tokens=None, title=None, artist=None, key=None, bpm=None, genre=None, moods=None, keywords=None, description=None, name=None, instrument=None, self_wav=WavCondition(wav=tensor([[[ 0.0001, -0.0003, -0.0002,  ...,  0.0041,  0.0040,  0.0040]]]), length=tensor([928000]), sample_rate=[32000], path=['/work/users/t/i/tis/V2Music/preprocessing/bg_audio/mdx_extra/C0o1BhJJ1ZI_no_vocals.wav'], seek_time=[0]), joint_embed={})\n",
      "batch_size: 2\n",
      "num_workers: 0\n",
      "Audio Tensor torch.Size([2, 1, 928000])\n",
      "Video Tensor torch.Size([2, 3, 58, 224, 224])\n",
      "Flow Tensor torch.Size([2, 1, 29, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(type(audio), type(video), type(info))\n",
    "vids, flows = video\n",
    "v_infos, f_infos = info\n",
    "print(v_infos)\n",
    "print(f_infos)\n",
    "print(\"batch_size:\", ld.batch_size)\n",
    "print(\"num_workers:\", ld.num_workers)\n",
    "print(\"Audio Tensor\", audio.shape)\n",
    "print(\"Video Tensor\", vids.shape)\n",
    "print(\"Flow Tensor\", flows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18f61027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/users/t/i/tis/VidMuse/.venv/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from audiocraft.models.builders import get_lm_model\n",
    "lm = get_lm_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84e3299c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual: torch.Size([2, 58, 768])\n",
      "Flow: torch.Size([2, 29, 768])\n"
     ]
    }
   ],
   "source": [
    "video_emb = lm.compute_video_emb(video, device=cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a5b03cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 58, 1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_emb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10dbda32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e04365d31b243aaa00b74a2bb9f2a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ac4713e7494260b1460e6bba58aa68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/236M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from audiocraft.solvers import CompressionSolver\n",
    "compression_model = CompressionSolver.wrapped_model_from_checkpoint(cfg, cfg.compression_model_checkpoint, device=cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4789ea7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m audio_tokens, scale \u001b[38;5;241m=\u001b[39m \u001b[43mcompression_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/users/t/i/tis/VidMuse/audiocraft/models/encodec.py:421\u001b[0m, in \u001b[0;36mHFEncodecCompressionModel.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    419\u001b[0m bandwidth_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpossible_num_codebooks\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_codebooks)\n\u001b[1;32m    420\u001b[0m bandwidth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtarget_bandwidths[bandwidth_index]\n\u001b[0;32m--> 421\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbandwidth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/work/users/t/i/tis/VidMuse/.venv/lib/python3.9/site-packages/transformers/models/encodec/modeling_encodec.py:650\u001b[0m, in \u001b[0;36mEncodecModel.encode\u001b[0;34m(self, input_values, padding_mask, bandwidth, return_dict)\u001b[0m\n\u001b[1;32m    648\u001b[0m mask \u001b[38;5;241m=\u001b[39m padding_mask[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, offset : offset \u001b[38;5;241m+\u001b[39m chunk_length]\u001b[38;5;241m.\u001b[39mbool()\n\u001b[1;32m    649\u001b[0m frame \u001b[38;5;241m=\u001b[39m input_values[:, :, offset : offset \u001b[38;5;241m+\u001b[39m chunk_length]\n\u001b[0;32m--> 650\u001b[0m encoded_frame, scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbandwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m encoded_frames\u001b[38;5;241m.\u001b[39mappend(encoded_frame)\n\u001b[1;32m    652\u001b[0m scales\u001b[38;5;241m.\u001b[39mappend(scale)\n",
      "File \u001b[0;32m/work/users/t/i/tis/VidMuse/.venv/lib/python3.9/site-packages/transformers/models/encodec/modeling_encodec.py:584\u001b[0m, in \u001b[0;36mEncodecModel._encode_frame\u001b[0;34m(self, input_values, bandwidth, padding_mask)\u001b[0m\n\u001b[1;32m    581\u001b[0m     scale \u001b[38;5;241m=\u001b[39m mono\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m\n\u001b[1;32m    582\u001b[0m     input_values \u001b[38;5;241m=\u001b[39m input_values \u001b[38;5;241m/\u001b[39m scale\n\u001b[0;32m--> 584\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m codes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantizer\u001b[38;5;241m.\u001b[39mencode(embeddings, bandwidth)\n\u001b[1;32m    586\u001b[0m codes \u001b[38;5;241m=\u001b[39m codes\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/work/users/t/i/tis/VidMuse/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/users/t/i/tis/VidMuse/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/work/users/t/i/tis/VidMuse/.venv/lib/python3.9/site-packages/transformers/models/encodec/modeling_encodec.py:312\u001b[0m, in \u001b[0;36mEncodecEncoder.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 312\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/work/users/t/i/tis/VidMuse/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/users/t/i/tis/VidMuse/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/work/users/t/i/tis/VidMuse/.venv/lib/python3.9/site-packages/transformers/models/encodec/modeling_encodec.py:171\u001b[0m, in \u001b[0;36mEncodecConv1d.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    166\u001b[0m     padding_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_total \u001b[38;5;241m-\u001b[39m padding_right\n\u001b[1;32m    167\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pad1d(\n\u001b[1;32m    168\u001b[0m         hidden_states, (padding_left, padding_right \u001b[38;5;241m+\u001b[39m extra_padding), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_mode\n\u001b[1;32m    169\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_group_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    174\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n",
      "File \u001b[0;32m/work/users/t/i/tis/VidMuse/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/users/t/i/tis/VidMuse/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/work/users/t/i/tis/VidMuse/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/users/t/i/tis/VidMuse/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "audio_tokens, scale = compression_model.encode(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaa5bd01",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Sequence shape must match the specified number of codebooks",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/users/t/i/tis/VidMuse/audiocraft/models/lm.py:357\u001b[0m, in \u001b[0;36mLMModel.forward\u001b[0;34m(self, sequence, conditions, video_tensor_list, precomputed_video_emb)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sequence: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    351\u001b[0m             conditions: tp\u001b[38;5;241m.\u001b[39mList[ConditioningAttributes],\n\u001b[1;32m    352\u001b[0m             video_tensor_list: tp\u001b[38;5;241m.\u001b[39mList,\n\u001b[1;32m    353\u001b[0m             precomputed_video_emb: tp\u001b[38;5;241m.\u001b[39mOptional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# 新增参数\u001b[39;00m\n\u001b[1;32m    354\u001b[0m             ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    356\u001b[0m     B, K, S \u001b[38;5;241m=\u001b[39m sequence\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m K \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_codebooks, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence shape must match the specified number of codebooks\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m     input_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb[k](sequence[:, k]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(K)])\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m input_\u001b[38;5;241m.\u001b[39mdevice\n",
      "\u001b[0;31mAssertionError\u001b[0m: Sequence shape must match the specified number of codebooks"
     ]
    }
   ],
   "source": [
    "lm.forward(audio, [], video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae444e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vidmuse",
   "language": "python",
   "name": "vidmuse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
