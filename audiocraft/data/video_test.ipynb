{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7577fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from video import video_read_local, video_read_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea9564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Read the JSONL file and extract video_paths into a list\n",
    "video_paths = []\n",
    "with open('/work/users/t/i/tis/VidMuse/egs/V2M20K/eval/test/data.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        video_paths.append(data['video_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddbec5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/users/t/i/tis/V2Music/preprocessing/data/video/9DF7rMJxvRo.mp4\n"
     ]
    }
   ],
   "source": [
    "print(video_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53935651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/users/t/i/tis/VidMuse/.venv/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "l, g = video_read_global(video_paths[0],duration=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35d0dbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 58, 224, 224]) torch.Size([3, 32, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(l.shape, g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "338cae33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/users/t/i/tis/VidMuse/.venv/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 58, 224, 224]) torch.Size([3, 32, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "l, g = video_read_global(video_paths[1], duration=30)\n",
    "print(l.shape, g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8219b555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 58, 224, 224]) torch.Size([3, 32, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "l, g = video_read_global(video_paths[2], duration=29)\n",
    "print(l.shape, g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1932ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ckpt = torch.load(\"/work/users/t/i/tis/VidMuse/model/state_dict.bin\")\n",
    "print(type(ckpt['best_state']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ced143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['local_pos_embedding', 'global_pos_embedding', 'condition_provider.conditioners.description.output_proj.weight', 'condition_provider.conditioners.description.output_proj.bias', 'emb.0.weight', 'emb.1.weight', 'emb.2.weight', 'emb.3.weight', 'transformer.layers.0.self_attn.in_proj_weight', 'transformer.layers.0.self_attn.out_proj.weight', 'transformer.layers.0.linear1.weight', 'transformer.layers.0.linear2.weight', 'transformer.layers.0.norm1.weight', 'transformer.layers.0.norm1.bias', 'transformer.layers.0.norm2.weight', 'transformer.layers.0.norm2.bias', 'transformer.layers.0.cross_attention.in_proj_weight', 'transformer.layers.0.cross_attention.out_proj.weight', 'transformer.layers.0.norm_cross.weight', 'transformer.layers.0.norm_cross.bias', 'transformer.layers.1.self_attn.in_proj_weight', 'transformer.layers.1.self_attn.out_proj.weight', 'transformer.layers.1.linear1.weight', 'transformer.layers.1.linear2.weight', 'transformer.layers.1.norm1.weight', 'transformer.layers.1.norm1.bias', 'transformer.layers.1.norm2.weight', 'transformer.layers.1.norm2.bias', 'transformer.layers.1.cross_attention.in_proj_weight', 'transformer.layers.1.cross_attention.out_proj.weight', 'transformer.layers.1.norm_cross.weight', 'transformer.layers.1.norm_cross.bias', 'transformer.layers.2.self_attn.in_proj_weight', 'transformer.layers.2.self_attn.out_proj.weight', 'transformer.layers.2.linear1.weight', 'transformer.layers.2.linear2.weight', 'transformer.layers.2.norm1.weight', 'transformer.layers.2.norm1.bias', 'transformer.layers.2.norm2.weight', 'transformer.layers.2.norm2.bias', 'transformer.layers.2.cross_attention.in_proj_weight', 'transformer.layers.2.cross_attention.out_proj.weight', 'transformer.layers.2.norm_cross.weight', 'transformer.layers.2.norm_cross.bias', 'transformer.layers.3.self_attn.in_proj_weight', 'transformer.layers.3.self_attn.out_proj.weight', 'transformer.layers.3.linear1.weight', 'transformer.layers.3.linear2.weight', 'transformer.layers.3.norm1.weight', 'transformer.layers.3.norm1.bias', 'transformer.layers.3.norm2.weight', 'transformer.layers.3.norm2.bias', 'transformer.layers.3.cross_attention.in_proj_weight', 'transformer.layers.3.cross_attention.out_proj.weight', 'transformer.layers.3.norm_cross.weight', 'transformer.layers.3.norm_cross.bias', 'transformer.layers.4.self_attn.in_proj_weight', 'transformer.layers.4.self_attn.out_proj.weight', 'transformer.layers.4.linear1.weight', 'transformer.layers.4.linear2.weight', 'transformer.layers.4.norm1.weight', 'transformer.layers.4.norm1.bias', 'transformer.layers.4.norm2.weight', 'transformer.layers.4.norm2.bias', 'transformer.layers.4.cross_attention.in_proj_weight', 'transformer.layers.4.cross_attention.out_proj.weight', 'transformer.layers.4.norm_cross.weight', 'transformer.layers.4.norm_cross.bias', 'transformer.layers.5.self_attn.in_proj_weight', 'transformer.layers.5.self_attn.out_proj.weight', 'transformer.layers.5.linear1.weight', 'transformer.layers.5.linear2.weight', 'transformer.layers.5.norm1.weight', 'transformer.layers.5.norm1.bias', 'transformer.layers.5.norm2.weight', 'transformer.layers.5.norm2.bias', 'transformer.layers.5.cross_attention.in_proj_weight', 'transformer.layers.5.cross_attention.out_proj.weight', 'transformer.layers.5.norm_cross.weight', 'transformer.layers.5.norm_cross.bias', 'transformer.layers.6.self_attn.in_proj_weight', 'transformer.layers.6.self_attn.out_proj.weight', 'transformer.layers.6.linear1.weight', 'transformer.layers.6.linear2.weight', 'transformer.layers.6.norm1.weight', 'transformer.layers.6.norm1.bias', 'transformer.layers.6.norm2.weight', 'transformer.layers.6.norm2.bias', 'transformer.layers.6.cross_attention.in_proj_weight', 'transformer.layers.6.cross_attention.out_proj.weight', 'transformer.layers.6.norm_cross.weight', 'transformer.layers.6.norm_cross.bias', 'transformer.layers.7.self_attn.in_proj_weight', 'transformer.layers.7.self_attn.out_proj.weight', 'transformer.layers.7.linear1.weight', 'transformer.layers.7.linear2.weight', 'transformer.layers.7.norm1.weight', 'transformer.layers.7.norm1.bias', 'transformer.layers.7.norm2.weight', 'transformer.layers.7.norm2.bias', 'transformer.layers.7.cross_attention.in_proj_weight', 'transformer.layers.7.cross_attention.out_proj.weight', 'transformer.layers.7.norm_cross.weight', 'transformer.layers.7.norm_cross.bias', 'transformer.layers.8.self_attn.in_proj_weight', 'transformer.layers.8.self_attn.out_proj.weight', 'transformer.layers.8.linear1.weight', 'transformer.layers.8.linear2.weight', 'transformer.layers.8.norm1.weight', 'transformer.layers.8.norm1.bias', 'transformer.layers.8.norm2.weight', 'transformer.layers.8.norm2.bias', 'transformer.layers.8.cross_attention.in_proj_weight', 'transformer.layers.8.cross_attention.out_proj.weight', 'transformer.layers.8.norm_cross.weight', 'transformer.layers.8.norm_cross.bias', 'transformer.layers.9.self_attn.in_proj_weight', 'transformer.layers.9.self_attn.out_proj.weight', 'transformer.layers.9.linear1.weight', 'transformer.layers.9.linear2.weight', 'transformer.layers.9.norm1.weight', 'transformer.layers.9.norm1.bias', 'transformer.layers.9.norm2.weight', 'transformer.layers.9.norm2.bias', 'transformer.layers.9.cross_attention.in_proj_weight', 'transformer.layers.9.cross_attention.out_proj.weight', 'transformer.layers.9.norm_cross.weight', 'transformer.layers.9.norm_cross.bias', 'transformer.layers.10.self_attn.in_proj_weight', 'transformer.layers.10.self_attn.out_proj.weight', 'transformer.layers.10.linear1.weight', 'transformer.layers.10.linear2.weight', 'transformer.layers.10.norm1.weight', 'transformer.layers.10.norm1.bias', 'transformer.layers.10.norm2.weight', 'transformer.layers.10.norm2.bias', 'transformer.layers.10.cross_attention.in_proj_weight', 'transformer.layers.10.cross_attention.out_proj.weight', 'transformer.layers.10.norm_cross.weight', 'transformer.layers.10.norm_cross.bias', 'transformer.layers.11.self_attn.in_proj_weight', 'transformer.layers.11.self_attn.out_proj.weight', 'transformer.layers.11.linear1.weight', 'transformer.layers.11.linear2.weight', 'transformer.layers.11.norm1.weight', 'transformer.layers.11.norm1.bias', 'transformer.layers.11.norm2.weight', 'transformer.layers.11.norm2.bias', 'transformer.layers.11.cross_attention.in_proj_weight', 'transformer.layers.11.cross_attention.out_proj.weight', 'transformer.layers.11.norm_cross.weight', 'transformer.layers.11.norm_cross.bias', 'transformer.layers.12.self_attn.in_proj_weight', 'transformer.layers.12.self_attn.out_proj.weight', 'transformer.layers.12.linear1.weight', 'transformer.layers.12.linear2.weight', 'transformer.layers.12.norm1.weight', 'transformer.layers.12.norm1.bias', 'transformer.layers.12.norm2.weight', 'transformer.layers.12.norm2.bias', 'transformer.layers.12.cross_attention.in_proj_weight', 'transformer.layers.12.cross_attention.out_proj.weight', 'transformer.layers.12.norm_cross.weight', 'transformer.layers.12.norm_cross.bias', 'transformer.layers.13.self_attn.in_proj_weight', 'transformer.layers.13.self_attn.out_proj.weight', 'transformer.layers.13.linear1.weight', 'transformer.layers.13.linear2.weight', 'transformer.layers.13.norm1.weight', 'transformer.layers.13.norm1.bias', 'transformer.layers.13.norm2.weight', 'transformer.layers.13.norm2.bias', 'transformer.layers.13.cross_attention.in_proj_weight', 'transformer.layers.13.cross_attention.out_proj.weight', 'transformer.layers.13.norm_cross.weight', 'transformer.layers.13.norm_cross.bias', 'transformer.layers.14.self_attn.in_proj_weight', 'transformer.layers.14.self_attn.out_proj.weight', 'transformer.layers.14.linear1.weight', 'transformer.layers.14.linear2.weight', 'transformer.layers.14.norm1.weight', 'transformer.layers.14.norm1.bias', 'transformer.layers.14.norm2.weight', 'transformer.layers.14.norm2.bias', 'transformer.layers.14.cross_attention.in_proj_weight', 'transformer.layers.14.cross_attention.out_proj.weight', 'transformer.layers.14.norm_cross.weight', 'transformer.layers.14.norm_cross.bias', 'transformer.layers.15.self_attn.in_proj_weight', 'transformer.layers.15.self_attn.out_proj.weight', 'transformer.layers.15.linear1.weight', 'transformer.layers.15.linear2.weight', 'transformer.layers.15.norm1.weight', 'transformer.layers.15.norm1.bias', 'transformer.layers.15.norm2.weight', 'transformer.layers.15.norm2.bias', 'transformer.layers.15.cross_attention.in_proj_weight', 'transformer.layers.15.cross_attention.out_proj.weight', 'transformer.layers.15.norm_cross.weight', 'transformer.layers.15.norm_cross.bias', 'transformer.layers.16.self_attn.in_proj_weight', 'transformer.layers.16.self_attn.out_proj.weight', 'transformer.layers.16.linear1.weight', 'transformer.layers.16.linear2.weight', 'transformer.layers.16.norm1.weight', 'transformer.layers.16.norm1.bias', 'transformer.layers.16.norm2.weight', 'transformer.layers.16.norm2.bias', 'transformer.layers.16.cross_attention.in_proj_weight', 'transformer.layers.16.cross_attention.out_proj.weight', 'transformer.layers.16.norm_cross.weight', 'transformer.layers.16.norm_cross.bias', 'transformer.layers.17.self_attn.in_proj_weight', 'transformer.layers.17.self_attn.out_proj.weight', 'transformer.layers.17.linear1.weight', 'transformer.layers.17.linear2.weight', 'transformer.layers.17.norm1.weight', 'transformer.layers.17.norm1.bias', 'transformer.layers.17.norm2.weight', 'transformer.layers.17.norm2.bias', 'transformer.layers.17.cross_attention.in_proj_weight', 'transformer.layers.17.cross_attention.out_proj.weight', 'transformer.layers.17.norm_cross.weight', 'transformer.layers.17.norm_cross.bias', 'transformer.layers.18.self_attn.in_proj_weight', 'transformer.layers.18.self_attn.out_proj.weight', 'transformer.layers.18.linear1.weight', 'transformer.layers.18.linear2.weight', 'transformer.layers.18.norm1.weight', 'transformer.layers.18.norm1.bias', 'transformer.layers.18.norm2.weight', 'transformer.layers.18.norm2.bias', 'transformer.layers.18.cross_attention.in_proj_weight', 'transformer.layers.18.cross_attention.out_proj.weight', 'transformer.layers.18.norm_cross.weight', 'transformer.layers.18.norm_cross.bias', 'transformer.layers.19.self_attn.in_proj_weight', 'transformer.layers.19.self_attn.out_proj.weight', 'transformer.layers.19.linear1.weight', 'transformer.layers.19.linear2.weight', 'transformer.layers.19.norm1.weight', 'transformer.layers.19.norm1.bias', 'transformer.layers.19.norm2.weight', 'transformer.layers.19.norm2.bias', 'transformer.layers.19.cross_attention.in_proj_weight', 'transformer.layers.19.cross_attention.out_proj.weight', 'transformer.layers.19.norm_cross.weight', 'transformer.layers.19.norm_cross.bias', 'transformer.layers.20.self_attn.in_proj_weight', 'transformer.layers.20.self_attn.out_proj.weight', 'transformer.layers.20.linear1.weight', 'transformer.layers.20.linear2.weight', 'transformer.layers.20.norm1.weight', 'transformer.layers.20.norm1.bias', 'transformer.layers.20.norm2.weight', 'transformer.layers.20.norm2.bias', 'transformer.layers.20.cross_attention.in_proj_weight', 'transformer.layers.20.cross_attention.out_proj.weight', 'transformer.layers.20.norm_cross.weight', 'transformer.layers.20.norm_cross.bias', 'transformer.layers.21.self_attn.in_proj_weight', 'transformer.layers.21.self_attn.out_proj.weight', 'transformer.layers.21.linear1.weight', 'transformer.layers.21.linear2.weight', 'transformer.layers.21.norm1.weight', 'transformer.layers.21.norm1.bias', 'transformer.layers.21.norm2.weight', 'transformer.layers.21.norm2.bias', 'transformer.layers.21.cross_attention.in_proj_weight', 'transformer.layers.21.cross_attention.out_proj.weight', 'transformer.layers.21.norm_cross.weight', 'transformer.layers.21.norm_cross.bias', 'transformer.layers.22.self_attn.in_proj_weight', 'transformer.layers.22.self_attn.out_proj.weight', 'transformer.layers.22.linear1.weight', 'transformer.layers.22.linear2.weight', 'transformer.layers.22.norm1.weight', 'transformer.layers.22.norm1.bias', 'transformer.layers.22.norm2.weight', 'transformer.layers.22.norm2.bias', 'transformer.layers.22.cross_attention.in_proj_weight', 'transformer.layers.22.cross_attention.out_proj.weight', 'transformer.layers.22.norm_cross.weight', 'transformer.layers.22.norm_cross.bias', 'transformer.layers.23.self_attn.in_proj_weight', 'transformer.layers.23.self_attn.out_proj.weight', 'transformer.layers.23.linear1.weight', 'transformer.layers.23.linear2.weight', 'transformer.layers.23.norm1.weight', 'transformer.layers.23.norm1.bias', 'transformer.layers.23.norm2.weight', 'transformer.layers.23.norm2.bias', 'transformer.layers.23.cross_attention.in_proj_weight', 'transformer.layers.23.cross_attention.out_proj.weight', 'transformer.layers.23.norm_cross.weight', 'transformer.layers.23.norm_cross.bias', 'transformer.layers.24.self_attn.in_proj_weight', 'transformer.layers.24.self_attn.out_proj.weight', 'transformer.layers.24.linear1.weight', 'transformer.layers.24.linear2.weight', 'transformer.layers.24.norm1.weight', 'transformer.layers.24.norm1.bias', 'transformer.layers.24.norm2.weight', 'transformer.layers.24.norm2.bias', 'transformer.layers.24.cross_attention.in_proj_weight', 'transformer.layers.24.cross_attention.out_proj.weight', 'transformer.layers.24.norm_cross.weight', 'transformer.layers.24.norm_cross.bias', 'transformer.layers.25.self_attn.in_proj_weight', 'transformer.layers.25.self_attn.out_proj.weight', 'transformer.layers.25.linear1.weight', 'transformer.layers.25.linear2.weight', 'transformer.layers.25.norm1.weight', 'transformer.layers.25.norm1.bias', 'transformer.layers.25.norm2.weight', 'transformer.layers.25.norm2.bias', 'transformer.layers.25.cross_attention.in_proj_weight', 'transformer.layers.25.cross_attention.out_proj.weight', 'transformer.layers.25.norm_cross.weight', 'transformer.layers.25.norm_cross.bias', 'transformer.layers.26.self_attn.in_proj_weight', 'transformer.layers.26.self_attn.out_proj.weight', 'transformer.layers.26.linear1.weight', 'transformer.layers.26.linear2.weight', 'transformer.layers.26.norm1.weight', 'transformer.layers.26.norm1.bias', 'transformer.layers.26.norm2.weight', 'transformer.layers.26.norm2.bias', 'transformer.layers.26.cross_attention.in_proj_weight', 'transformer.layers.26.cross_attention.out_proj.weight', 'transformer.layers.26.norm_cross.weight', 'transformer.layers.26.norm_cross.bias', 'transformer.layers.27.self_attn.in_proj_weight', 'transformer.layers.27.self_attn.out_proj.weight', 'transformer.layers.27.linear1.weight', 'transformer.layers.27.linear2.weight', 'transformer.layers.27.norm1.weight', 'transformer.layers.27.norm1.bias', 'transformer.layers.27.norm2.weight', 'transformer.layers.27.norm2.bias', 'transformer.layers.27.cross_attention.in_proj_weight', 'transformer.layers.27.cross_attention.out_proj.weight', 'transformer.layers.27.norm_cross.weight', 'transformer.layers.27.norm_cross.bias', 'transformer.layers.28.self_attn.in_proj_weight', 'transformer.layers.28.self_attn.out_proj.weight', 'transformer.layers.28.linear1.weight', 'transformer.layers.28.linear2.weight', 'transformer.layers.28.norm1.weight', 'transformer.layers.28.norm1.bias', 'transformer.layers.28.norm2.weight', 'transformer.layers.28.norm2.bias', 'transformer.layers.28.cross_attention.in_proj_weight', 'transformer.layers.28.cross_attention.out_proj.weight', 'transformer.layers.28.norm_cross.weight', 'transformer.layers.28.norm_cross.bias', 'transformer.layers.29.self_attn.in_proj_weight', 'transformer.layers.29.self_attn.out_proj.weight', 'transformer.layers.29.linear1.weight', 'transformer.layers.29.linear2.weight', 'transformer.layers.29.norm1.weight', 'transformer.layers.29.norm1.bias', 'transformer.layers.29.norm2.weight', 'transformer.layers.29.norm2.bias', 'transformer.layers.29.cross_attention.in_proj_weight', 'transformer.layers.29.cross_attention.out_proj.weight', 'transformer.layers.29.norm_cross.weight', 'transformer.layers.29.norm_cross.bias', 'transformer.layers.30.self_attn.in_proj_weight', 'transformer.layers.30.self_attn.out_proj.weight', 'transformer.layers.30.linear1.weight', 'transformer.layers.30.linear2.weight', 'transformer.layers.30.norm1.weight', 'transformer.layers.30.norm1.bias', 'transformer.layers.30.norm2.weight', 'transformer.layers.30.norm2.bias', 'transformer.layers.30.cross_attention.in_proj_weight', 'transformer.layers.30.cross_attention.out_proj.weight', 'transformer.layers.30.norm_cross.weight', 'transformer.layers.30.norm_cross.bias', 'transformer.layers.31.self_attn.in_proj_weight', 'transformer.layers.31.self_attn.out_proj.weight', 'transformer.layers.31.linear1.weight', 'transformer.layers.31.linear2.weight', 'transformer.layers.31.norm1.weight', 'transformer.layers.31.norm1.bias', 'transformer.layers.31.norm2.weight', 'transformer.layers.31.norm2.bias', 'transformer.layers.31.cross_attention.in_proj_weight', 'transformer.layers.31.cross_attention.out_proj.weight', 'transformer.layers.31.norm_cross.weight', 'transformer.layers.31.norm_cross.bias', 'transformer.layers.32.self_attn.in_proj_weight', 'transformer.layers.32.self_attn.out_proj.weight', 'transformer.layers.32.linear1.weight', 'transformer.layers.32.linear2.weight', 'transformer.layers.32.norm1.weight', 'transformer.layers.32.norm1.bias', 'transformer.layers.32.norm2.weight', 'transformer.layers.32.norm2.bias', 'transformer.layers.32.cross_attention.in_proj_weight', 'transformer.layers.32.cross_attention.out_proj.weight', 'transformer.layers.32.norm_cross.weight', 'transformer.layers.32.norm_cross.bias', 'transformer.layers.33.self_attn.in_proj_weight', 'transformer.layers.33.self_attn.out_proj.weight', 'transformer.layers.33.linear1.weight', 'transformer.layers.33.linear2.weight', 'transformer.layers.33.norm1.weight', 'transformer.layers.33.norm1.bias', 'transformer.layers.33.norm2.weight', 'transformer.layers.33.norm2.bias', 'transformer.layers.33.cross_attention.in_proj_weight', 'transformer.layers.33.cross_attention.out_proj.weight', 'transformer.layers.33.norm_cross.weight', 'transformer.layers.33.norm_cross.bias', 'transformer.layers.34.self_attn.in_proj_weight', 'transformer.layers.34.self_attn.out_proj.weight', 'transformer.layers.34.linear1.weight', 'transformer.layers.34.linear2.weight', 'transformer.layers.34.norm1.weight', 'transformer.layers.34.norm1.bias', 'transformer.layers.34.norm2.weight', 'transformer.layers.34.norm2.bias', 'transformer.layers.34.cross_attention.in_proj_weight', 'transformer.layers.34.cross_attention.out_proj.weight', 'transformer.layers.34.norm_cross.weight', 'transformer.layers.34.norm_cross.bias', 'transformer.layers.35.self_attn.in_proj_weight', 'transformer.layers.35.self_attn.out_proj.weight', 'transformer.layers.35.linear1.weight', 'transformer.layers.35.linear2.weight', 'transformer.layers.35.norm1.weight', 'transformer.layers.35.norm1.bias', 'transformer.layers.35.norm2.weight', 'transformer.layers.35.norm2.bias', 'transformer.layers.35.cross_attention.in_proj_weight', 'transformer.layers.35.cross_attention.out_proj.weight', 'transformer.layers.35.norm_cross.weight', 'transformer.layers.35.norm_cross.bias', 'transformer.layers.36.self_attn.in_proj_weight', 'transformer.layers.36.self_attn.out_proj.weight', 'transformer.layers.36.linear1.weight', 'transformer.layers.36.linear2.weight', 'transformer.layers.36.norm1.weight', 'transformer.layers.36.norm1.bias', 'transformer.layers.36.norm2.weight', 'transformer.layers.36.norm2.bias', 'transformer.layers.36.cross_attention.in_proj_weight', 'transformer.layers.36.cross_attention.out_proj.weight', 'transformer.layers.36.norm_cross.weight', 'transformer.layers.36.norm_cross.bias', 'transformer.layers.37.self_attn.in_proj_weight', 'transformer.layers.37.self_attn.out_proj.weight', 'transformer.layers.37.linear1.weight', 'transformer.layers.37.linear2.weight', 'transformer.layers.37.norm1.weight', 'transformer.layers.37.norm1.bias', 'transformer.layers.37.norm2.weight', 'transformer.layers.37.norm2.bias', 'transformer.layers.37.cross_attention.in_proj_weight', 'transformer.layers.37.cross_attention.out_proj.weight', 'transformer.layers.37.norm_cross.weight', 'transformer.layers.37.norm_cross.bias', 'transformer.layers.38.self_attn.in_proj_weight', 'transformer.layers.38.self_attn.out_proj.weight', 'transformer.layers.38.linear1.weight', 'transformer.layers.38.linear2.weight', 'transformer.layers.38.norm1.weight', 'transformer.layers.38.norm1.bias', 'transformer.layers.38.norm2.weight', 'transformer.layers.38.norm2.bias', 'transformer.layers.38.cross_attention.in_proj_weight', 'transformer.layers.38.cross_attention.out_proj.weight', 'transformer.layers.38.norm_cross.weight', 'transformer.layers.38.norm_cross.bias', 'transformer.layers.39.self_attn.in_proj_weight', 'transformer.layers.39.self_attn.out_proj.weight', 'transformer.layers.39.linear1.weight', 'transformer.layers.39.linear2.weight', 'transformer.layers.39.norm1.weight', 'transformer.layers.39.norm1.bias', 'transformer.layers.39.norm2.weight', 'transformer.layers.39.norm2.bias', 'transformer.layers.39.cross_attention.in_proj_weight', 'transformer.layers.39.cross_attention.out_proj.weight', 'transformer.layers.39.norm_cross.weight', 'transformer.layers.39.norm_cross.bias', 'transformer.layers.40.self_attn.in_proj_weight', 'transformer.layers.40.self_attn.out_proj.weight', 'transformer.layers.40.linear1.weight', 'transformer.layers.40.linear2.weight', 'transformer.layers.40.norm1.weight', 'transformer.layers.40.norm1.bias', 'transformer.layers.40.norm2.weight', 'transformer.layers.40.norm2.bias', 'transformer.layers.40.cross_attention.in_proj_weight', 'transformer.layers.40.cross_attention.out_proj.weight', 'transformer.layers.40.norm_cross.weight', 'transformer.layers.40.norm_cross.bias', 'transformer.layers.41.self_attn.in_proj_weight', 'transformer.layers.41.self_attn.out_proj.weight', 'transformer.layers.41.linear1.weight', 'transformer.layers.41.linear2.weight', 'transformer.layers.41.norm1.weight', 'transformer.layers.41.norm1.bias', 'transformer.layers.41.norm2.weight', 'transformer.layers.41.norm2.bias', 'transformer.layers.41.cross_attention.in_proj_weight', 'transformer.layers.41.cross_attention.out_proj.weight', 'transformer.layers.41.norm_cross.weight', 'transformer.layers.41.norm_cross.bias', 'transformer.layers.42.self_attn.in_proj_weight', 'transformer.layers.42.self_attn.out_proj.weight', 'transformer.layers.42.linear1.weight', 'transformer.layers.42.linear2.weight', 'transformer.layers.42.norm1.weight', 'transformer.layers.42.norm1.bias', 'transformer.layers.42.norm2.weight', 'transformer.layers.42.norm2.bias', 'transformer.layers.42.cross_attention.in_proj_weight', 'transformer.layers.42.cross_attention.out_proj.weight', 'transformer.layers.42.norm_cross.weight', 'transformer.layers.42.norm_cross.bias', 'transformer.layers.43.self_attn.in_proj_weight', 'transformer.layers.43.self_attn.out_proj.weight', 'transformer.layers.43.linear1.weight', 'transformer.layers.43.linear2.weight', 'transformer.layers.43.norm1.weight', 'transformer.layers.43.norm1.bias', 'transformer.layers.43.norm2.weight', 'transformer.layers.43.norm2.bias', 'transformer.layers.43.cross_attention.in_proj_weight', 'transformer.layers.43.cross_attention.out_proj.weight', 'transformer.layers.43.norm_cross.weight', 'transformer.layers.43.norm_cross.bias', 'transformer.layers.44.self_attn.in_proj_weight', 'transformer.layers.44.self_attn.out_proj.weight', 'transformer.layers.44.linear1.weight', 'transformer.layers.44.linear2.weight', 'transformer.layers.44.norm1.weight', 'transformer.layers.44.norm1.bias', 'transformer.layers.44.norm2.weight', 'transformer.layers.44.norm2.bias', 'transformer.layers.44.cross_attention.in_proj_weight', 'transformer.layers.44.cross_attention.out_proj.weight', 'transformer.layers.44.norm_cross.weight', 'transformer.layers.44.norm_cross.bias', 'transformer.layers.45.self_attn.in_proj_weight', 'transformer.layers.45.self_attn.out_proj.weight', 'transformer.layers.45.linear1.weight', 'transformer.layers.45.linear2.weight', 'transformer.layers.45.norm1.weight', 'transformer.layers.45.norm1.bias', 'transformer.layers.45.norm2.weight', 'transformer.layers.45.norm2.bias', 'transformer.layers.45.cross_attention.in_proj_weight', 'transformer.layers.45.cross_attention.out_proj.weight', 'transformer.layers.45.norm_cross.weight', 'transformer.layers.45.norm_cross.bias', 'transformer.layers.46.self_attn.in_proj_weight', 'transformer.layers.46.self_attn.out_proj.weight', 'transformer.layers.46.linear1.weight', 'transformer.layers.46.linear2.weight', 'transformer.layers.46.norm1.weight', 'transformer.layers.46.norm1.bias', 'transformer.layers.46.norm2.weight', 'transformer.layers.46.norm2.bias', 'transformer.layers.46.cross_attention.in_proj_weight', 'transformer.layers.46.cross_attention.out_proj.weight', 'transformer.layers.46.norm_cross.weight', 'transformer.layers.46.norm_cross.bias', 'transformer.layers.47.self_attn.in_proj_weight', 'transformer.layers.47.self_attn.out_proj.weight', 'transformer.layers.47.linear1.weight', 'transformer.layers.47.linear2.weight', 'transformer.layers.47.norm1.weight', 'transformer.layers.47.norm1.bias', 'transformer.layers.47.norm2.weight', 'transformer.layers.47.norm2.bias', 'transformer.layers.47.cross_attention.in_proj_weight', 'transformer.layers.47.cross_attention.out_proj.weight', 'transformer.layers.47.norm_cross.weight', 'transformer.layers.47.norm_cross.bias', 'out_norm.weight', 'out_norm.bias', 'linears.0.weight', 'linears.1.weight', 'linears.2.weight', 'linears.3.weight', 'visual_encoder_model.vision_model.embeddings.class_embedding', 'visual_encoder_model.vision_model.embeddings.patch_embedding.weight', 'visual_encoder_model.vision_model.embeddings.position_embedding.weight', 'visual_encoder_model.vision_model.pre_layrnorm.weight', 'visual_encoder_model.vision_model.pre_layrnorm.bias', 'visual_encoder_model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.0.layer_norm1.weight', 'visual_encoder_model.vision_model.encoder.layers.0.layer_norm1.bias', 'visual_encoder_model.vision_model.encoder.layers.0.mlp.fc1.weight', 'visual_encoder_model.vision_model.encoder.layers.0.mlp.fc1.bias', 'visual_encoder_model.vision_model.encoder.layers.0.mlp.fc2.weight', 'visual_encoder_model.vision_model.encoder.layers.0.mlp.fc2.bias', 'visual_encoder_model.vision_model.encoder.layers.0.layer_norm2.weight', 'visual_encoder_model.vision_model.encoder.layers.0.layer_norm2.bias', 'visual_encoder_model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.1.layer_norm1.weight', 'visual_encoder_model.vision_model.encoder.layers.1.layer_norm1.bias', 'visual_encoder_model.vision_model.encoder.layers.1.mlp.fc1.weight', 'visual_encoder_model.vision_model.encoder.layers.1.mlp.fc1.bias', 'visual_encoder_model.vision_model.encoder.layers.1.mlp.fc2.weight', 'visual_encoder_model.vision_model.encoder.layers.1.mlp.fc2.bias', 'visual_encoder_model.vision_model.encoder.layers.1.layer_norm2.weight', 'visual_encoder_model.vision_model.encoder.layers.1.layer_norm2.bias', 'visual_encoder_model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.2.layer_norm1.weight', 'visual_encoder_model.vision_model.encoder.layers.2.layer_norm1.bias', 'visual_encoder_model.vision_model.encoder.layers.2.mlp.fc1.weight', 'visual_encoder_model.vision_model.encoder.layers.2.mlp.fc1.bias', 'visual_encoder_model.vision_model.encoder.layers.2.mlp.fc2.weight', 'visual_encoder_model.vision_model.encoder.layers.2.mlp.fc2.bias', 'visual_encoder_model.vision_model.encoder.layers.2.layer_norm2.weight', 'visual_encoder_model.vision_model.encoder.layers.2.layer_norm2.bias', 'visual_encoder_model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.3.layer_norm1.weight', 'visual_encoder_model.vision_model.encoder.layers.3.layer_norm1.bias', 'visual_encoder_model.vision_model.encoder.layers.3.mlp.fc1.weight', 'visual_encoder_model.vision_model.encoder.layers.3.mlp.fc1.bias', 'visual_encoder_model.vision_model.encoder.layers.3.mlp.fc2.weight', 'visual_encoder_model.vision_model.encoder.layers.3.mlp.fc2.bias', 'visual_encoder_model.vision_model.encoder.layers.3.layer_norm2.weight', 'visual_encoder_model.vision_model.encoder.layers.3.layer_norm2.bias', 'visual_encoder_model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.4.layer_norm1.weight', 'visual_encoder_model.vision_model.encoder.layers.4.layer_norm1.bias', 'visual_encoder_model.vision_model.encoder.layers.4.mlp.fc1.weight', 'visual_encoder_model.vision_model.encoder.layers.4.mlp.fc1.bias', 'visual_encoder_model.vision_model.encoder.layers.4.mlp.fc2.weight', 'visual_encoder_model.vision_model.encoder.layers.4.mlp.fc2.bias', 'visual_encoder_model.vision_model.encoder.layers.4.layer_norm2.weight', 'visual_encoder_model.vision_model.encoder.layers.4.layer_norm2.bias', 'visual_encoder_model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.5.layer_norm1.weight', 'visual_encoder_model.vision_model.encoder.layers.5.layer_norm1.bias', 'visual_encoder_model.vision_model.encoder.layers.5.mlp.fc1.weight', 'visual_encoder_model.vision_model.encoder.layers.5.mlp.fc1.bias', 'visual_encoder_model.vision_model.encoder.layers.5.mlp.fc2.weight', 'visual_encoder_model.vision_model.encoder.layers.5.mlp.fc2.bias', 'visual_encoder_model.vision_model.encoder.layers.5.layer_norm2.weight', 'visual_encoder_model.vision_model.encoder.layers.5.layer_norm2.bias', 'visual_encoder_model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.6.layer_norm1.weight', 'visual_encoder_model.vision_model.encoder.layers.6.layer_norm1.bias', 'visual_encoder_model.vision_model.encoder.layers.6.mlp.fc1.weight', 'visual_encoder_model.vision_model.encoder.layers.6.mlp.fc1.bias', 'visual_encoder_model.vision_model.encoder.layers.6.mlp.fc2.weight', 'visual_encoder_model.vision_model.encoder.layers.6.mlp.fc2.bias', 'visual_encoder_model.vision_model.encoder.layers.6.layer_norm2.weight', 'visual_encoder_model.vision_model.encoder.layers.6.layer_norm2.bias', 'visual_encoder_model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.7.layer_norm1.weight', 'visual_encoder_model.vision_model.encoder.layers.7.layer_norm1.bias', 'visual_encoder_model.vision_model.encoder.layers.7.mlp.fc1.weight', 'visual_encoder_model.vision_model.encoder.layers.7.mlp.fc1.bias', 'visual_encoder_model.vision_model.encoder.layers.7.mlp.fc2.weight', 'visual_encoder_model.vision_model.encoder.layers.7.mlp.fc2.bias', 'visual_encoder_model.vision_model.encoder.layers.7.layer_norm2.weight', 'visual_encoder_model.vision_model.encoder.layers.7.layer_norm2.bias', 'visual_encoder_model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.8.layer_norm1.weight', 'visual_encoder_model.vision_model.encoder.layers.8.layer_norm1.bias', 'visual_encoder_model.vision_model.encoder.layers.8.mlp.fc1.weight', 'visual_encoder_model.vision_model.encoder.layers.8.mlp.fc1.bias', 'visual_encoder_model.vision_model.encoder.layers.8.mlp.fc2.weight', 'visual_encoder_model.vision_model.encoder.layers.8.mlp.fc2.bias', 'visual_encoder_model.vision_model.encoder.layers.8.layer_norm2.weight', 'visual_encoder_model.vision_model.encoder.layers.8.layer_norm2.bias', 'visual_encoder_model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.9.layer_norm1.weight', 'visual_encoder_model.vision_model.encoder.layers.9.layer_norm1.bias', 'visual_encoder_model.vision_model.encoder.layers.9.mlp.fc1.weight', 'visual_encoder_model.vision_model.encoder.layers.9.mlp.fc1.bias', 'visual_encoder_model.vision_model.encoder.layers.9.mlp.fc2.weight', 'visual_encoder_model.vision_model.encoder.layers.9.mlp.fc2.bias', 'visual_encoder_model.vision_model.encoder.layers.9.layer_norm2.weight', 'visual_encoder_model.vision_model.encoder.layers.9.layer_norm2.bias', 'visual_encoder_model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.10.layer_norm1.weight', 'visual_encoder_model.vision_model.encoder.layers.10.layer_norm1.bias', 'visual_encoder_model.vision_model.encoder.layers.10.mlp.fc1.weight', 'visual_encoder_model.vision_model.encoder.layers.10.mlp.fc1.bias', 'visual_encoder_model.vision_model.encoder.layers.10.mlp.fc2.weight', 'visual_encoder_model.vision_model.encoder.layers.10.mlp.fc2.bias', 'visual_encoder_model.vision_model.encoder.layers.10.layer_norm2.weight', 'visual_encoder_model.vision_model.encoder.layers.10.layer_norm2.bias', 'visual_encoder_model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'visual_encoder_model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'visual_encoder_model.vision_model.encoder.layers.11.layer_norm1.weight', 'visual_encoder_model.vision_model.encoder.layers.11.layer_norm1.bias', 'visual_encoder_model.vision_model.encoder.layers.11.mlp.fc1.weight', 'visual_encoder_model.vision_model.encoder.layers.11.mlp.fc1.bias', 'visual_encoder_model.vision_model.encoder.layers.11.mlp.fc2.weight', 'visual_encoder_model.vision_model.encoder.layers.11.mlp.fc2.bias', 'visual_encoder_model.vision_model.encoder.layers.11.layer_norm2.weight', 'visual_encoder_model.vision_model.encoder.layers.11.layer_norm2.bias', 'visual_encoder_model.vision_model.post_layernorm.weight', 'visual_encoder_model.vision_model.post_layernorm.bias', 'visual_encoder_model.visual_projection.weight', 'local_temporal_transformer.layers.0.0.norm.weight', 'local_temporal_transformer.layers.0.0.norm.bias', 'local_temporal_transformer.layers.0.0.fn.to_qkv.weight', 'local_temporal_transformer.layers.0.0.fn.to_out.0.weight', 'local_temporal_transformer.layers.0.0.fn.to_out.0.bias', 'local_temporal_transformer.layers.0.1.norm.weight', 'local_temporal_transformer.layers.0.1.norm.bias', 'local_temporal_transformer.layers.0.1.fn.net.0.weight', 'local_temporal_transformer.layers.0.1.fn.net.0.bias', 'local_temporal_transformer.layers.0.1.fn.net.3.weight', 'local_temporal_transformer.layers.0.1.fn.net.3.bias', 'local_temporal_transformer.layers.1.0.norm.weight', 'local_temporal_transformer.layers.1.0.norm.bias', 'local_temporal_transformer.layers.1.0.fn.to_qkv.weight', 'local_temporal_transformer.layers.1.0.fn.to_out.0.weight', 'local_temporal_transformer.layers.1.0.fn.to_out.0.bias', 'local_temporal_transformer.layers.1.1.norm.weight', 'local_temporal_transformer.layers.1.1.norm.bias', 'local_temporal_transformer.layers.1.1.fn.net.0.weight', 'local_temporal_transformer.layers.1.1.fn.net.0.bias', 'local_temporal_transformer.layers.1.1.fn.net.3.weight', 'local_temporal_transformer.layers.1.1.fn.net.3.bias', 'local_temporal_transformer.norm.weight', 'local_temporal_transformer.norm.bias', 'global_temporal_transformer.layers.0.0.norm.weight', 'global_temporal_transformer.layers.0.0.norm.bias', 'global_temporal_transformer.layers.0.0.fn.to_qkv.weight', 'global_temporal_transformer.layers.0.0.fn.to_out.0.weight', 'global_temporal_transformer.layers.0.0.fn.to_out.0.bias', 'global_temporal_transformer.layers.0.1.norm.weight', 'global_temporal_transformer.layers.0.1.norm.bias', 'global_temporal_transformer.layers.0.1.fn.net.0.weight', 'global_temporal_transformer.layers.0.1.fn.net.0.bias', 'global_temporal_transformer.layers.0.1.fn.net.3.weight', 'global_temporal_transformer.layers.0.1.fn.net.3.bias', 'global_temporal_transformer.layers.1.0.norm.weight', 'global_temporal_transformer.layers.1.0.norm.bias', 'global_temporal_transformer.layers.1.0.fn.to_qkv.weight', 'global_temporal_transformer.layers.1.0.fn.to_out.0.weight', 'global_temporal_transformer.layers.1.0.fn.to_out.0.bias', 'global_temporal_transformer.layers.1.1.norm.weight', 'global_temporal_transformer.layers.1.1.norm.bias', 'global_temporal_transformer.layers.1.1.fn.net.0.weight', 'global_temporal_transformer.layers.1.1.fn.net.0.bias', 'global_temporal_transformer.layers.1.1.fn.net.3.weight', 'global_temporal_transformer.layers.1.1.fn.net.3.bias', 'global_temporal_transformer.norm.weight', 'global_temporal_transformer.norm.bias', 'multi_head_cross_attention.query.weight', 'multi_head_cross_attention.query.bias', 'multi_head_cross_attention.key.weight', 'multi_head_cross_attention.key.bias', 'multi_head_cross_attention.value.weight', 'multi_head_cross_attention.value.bias', 'multi_head_cross_attention.final_linear.weight', 'multi_head_cross_attention.final_linear.bias', 'multi_head_cross_attention.norm1.weight', 'multi_head_cross_attention.norm1.bias', 'multi_head_cross_attention.norm2.weight', 'multi_head_cross_attention.norm2.bias', 'visual_feature_proj.weight', 'visual_feature_proj.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(ckpt['best_state'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128e99f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a945f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved fixed checkpoint to: /work/users/t/i/tis/VidMuse/model/state_dict_fixed.bin\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "input_path = \"/work/users/t/i/tis/VidMuse/model/state_dict.bin\"\n",
    "output_path = \"/work/users/t/i/tis/VidMuse/model/state_dict_fixed.bin\"\n",
    "\n",
    "# Load the original checkpoint\n",
    "ckpt = torch.load(input_path)\n",
    "\n",
    "\n",
    "wrapped_ckpt = {'best_state': {'model': ckpt['best_state']}}\n",
    "\n",
    "# Save the fixed checkpoint\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "torch.save(wrapped_ckpt, output_path)\n",
    "print(f\"✅ Saved fixed checkpoint to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db358b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'loaders' from 'audiocraft.models' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maudiocraft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m loaders\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'loaders' from 'audiocraft.models' (unknown location)"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
